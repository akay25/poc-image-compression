{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Config\n",
    "IMAGE_PATH = \"./dataset/\"\n",
    "MAX_IMAGES_TO_LOAD = 1500\n",
    "DENSE_BLOCK_SIZE = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.layers\n",
    "import tensorflow.keras.models\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.datasets\n",
    "import tensorflow.keras.optimizers \n",
    "from os import walk\n",
    "import numpy\n",
    "from timeit import default_timer as timer\n",
    "from keras.preprocessing.image import load_img \n",
    "import warnings \n",
    "from keras.preprocessing.image import img_to_array \n",
    "from keras.preprocessing.image import array_to_img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(np):\n",
    "    return np.size * np.itemsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading manga images in the memory\n",
    "images = []\n",
    "for (dirpath, dirnames, filenames) in walk(IMAGE_PATH):\n",
    "    images.extend(filenames)\n",
    "    break\n",
    "\n",
    "mangas = []\n",
    "for i,img in enumerate(images):\n",
    "    img_path = f\"{IMAGE_PATH}{img}\"\n",
    "    img_data = load_img(img_path)\n",
    "    mangas.append(img_to_array(img_data))\n",
    "    if i > MAX_IMAGES_TO_LOAD:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get max length of the image\n",
    "max_linear_size = -1\n",
    "original_shape = None\n",
    "for m in mangas:\n",
    "    h, w, d = m.shape\n",
    "    m_length = h * w * d\n",
    "    if m_length > max_linear_size:\n",
    "        original_shape = m.shape\n",
    "        max_linear_size = m_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1637, 690, 3)\n",
      "3388590\n"
     ]
    }
   ],
   "source": [
    "print(original_shape)\n",
    "print(max_linear_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3388590)\n"
     ]
    }
   ],
   "source": [
    "images = None\n",
    "\n",
    "for i, m in enumerate(mangas):\n",
    "    temp = m.reshape(-1)\n",
    "\n",
    "    if temp.shape[0] < max_linear_size:\n",
    "        padded_array = numpy.zeros((max_linear_size))\n",
    "        padded_array[:temp.shape[0]] = temp\n",
    "\n",
    "        if i == 0:\n",
    "            images = padded_array\n",
    "        else:\n",
    "            images = numpy.vstack((images, padded_array))\n",
    "\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tensorflow.keras.layers.Input(shape=(max_linear_size), name=\"encoder_input\")\n",
    "\n",
    "encoder_dense_layer1 = tensorflow.keras.layers.Dense(units=30, name=\"encoder_dense_1\")(x)\n",
    "encoder_activ_layer1 = tensorflow.keras.layers.LeakyReLU(name=\"encoder_leakyrelu_1\")(encoder_dense_layer1)\n",
    "\n",
    "encoder_dense_layer2 = tensorflow.keras.layers.Dense(units=DENSE_BLOCK_SIZE, name=\"encoder_dense_2\")(encoder_activ_layer1)\n",
    "encoder_output = tensorflow.keras.layers.LeakyReLU(name=\"encoder_output\")(encoder_dense_layer2)\n",
    "\n",
    "encoder = tensorflow.keras.models.Model(x, encoder_output, name=\"encoder_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = tensorflow.keras.layers.Input(shape=(DENSE_BLOCK_SIZE), name=\"decoder_input\")\n",
    "\n",
    "decoder_dense_layer1 = tensorflow.keras.layers.Dense(units=30, name=\"decoder_dense_1\")(decoder_input)\n",
    "decoder_activ_layer1 = tensorflow.keras.layers.LeakyReLU(name=\"decoder_leakyrelu_1\")(decoder_dense_layer1)\n",
    "\n",
    "decoder_dense_layer2 = tensorflow.keras.layers.Dense(units=max_linear_size , name=\"decoder_dense_2\")(decoder_activ_layer1)\n",
    "decoder_output = tensorflow.keras.layers.LeakyReLU(name=\"decoder_output\")(decoder_dense_layer2)\n",
    "\n",
    "decoder = tensorflow.keras.models.Model(decoder_input, decoder_output, name=\"decoder_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_input = tensorflow.keras.layers.Input(shape=(max_linear_size), name=\"AE_input\")\n",
    "ae_encoder_output = encoder(ae_input)\n",
    "ae_decoder_output = decoder(ae_encoder_output)\n",
    "\n",
    "ae = tensorflow.keras.models.Model(ae_input, ae_decoder_output, name=\"AE\")\n",
    "ae.compile(loss=\"mse\", optimizer=tensorflow.keras.optimizers.Adam(lr=0.0005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.shuffle(images)\n",
    "x_train, x_test = images[:80,:], images[80:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5 samples\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[30,3388590] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_2/Adam/Adam/update_decoder_dense_2_2/kernel/ResourceApplyAdam}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-d61a4a3bc432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[30,3388590] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_2/Adam/Adam/update_decoder_dense_2_2/kernel/ResourceApplyAdam}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "ae.fit(x_train, x_train, epochs=120,batch_size=256, shuffle=True, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_images = encoder.predict(x_train)\n",
    "decoded_images = decoder.predict(encoded_images)\n",
    "\n",
    "decoded_images_orig = numpy.reshape(decoded_images, newshape=(decoded_images.shape[0], 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(random_number):\n",
    "    num_images_to_show = 1\n",
    "    plt.subplot(num_images_to_show, 2, 1)\n",
    "    plt.imshow(x_train_orig[random_number, :, :], cmap=\"gray\")\n",
    "    plt.subplot(num_images_to_show, 2, 2)\n",
    "    plt.imshow(decoded_images_orig[random_number, :, :], cmap=\"gray\")\n",
    "    print(f\"Original =>{get_size(x_train_orig[random_number, :, :])}\")\n",
    "    print(f\"Encoded =>{get_size(encoded_images[random_number])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(encoded_image):\n",
    "    num_images_to_show = 1\n",
    "    start = timer()\n",
    "    decoded_image = decoder.predict([encoded_image])\n",
    "    decoded_image_orig = numpy.reshape(decoded_image, newshape=(decoded_image.shape[0], 28, 28))\n",
    "    plt.subplot(num_images_to_show, 2, 1)\n",
    "    plt.imshow(decoded_image_orig[0, :, :], cmap=\"gray\")\n",
    "    end = timer()\n",
    "    print(f\"Time taken to decompress => {end - start} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original =>3136\n",
      "Encoded =>32\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to decompress => 0.02045366000015747 secs\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALoAAAC4CAYAAABUxvb6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAP8UlEQVR4nO2dW4xV15GG/6JtfMM2bsDtNheDMVhqX4JtbMeayLKVYcSMI5G8oPhhxEM05CGWEikvyC+JIkXKw2QuD6NIRIPDSDNOgmYY0CiacdQamcSyrAYrCk4I5mYMuC+AsaGNbzSVh7M76vT+y5x19rmy/k9q9TnVdfaufU71Pqtq1apl7g4hrnbmdNoAIdqBHF1kgRxdZIEcXWSBHF1kgRxdZEElRzez9WZ20MwOm9mWZhklRLOxRvPoZtYH4E0A6wCcBDAC4Fl3//1nvMbnzNGXiGgNly9fhrsb+9s1FY77GIDD7n4UAMzspwA2AAgdfc6cOZg3b16FU169XLp0qSQzo58Z+vr6Kp0rurlF56uXy5cvU3nVm1u99k5OTobHqGLBYgAnZjw/WchmG7PZzPaa2V7NwopOUeWOXhfuvhXAVgDo6+uTp4uOUMXRTwFYOuP5kkJ21VD1Gyj6ymbHTfl6Z6+Phg1Mfs01/GNPuV42zGlV/FV1SAVUG7qMAFhlZivMbC6ArwLYXdkiIVpAw3d0d79kZs8B+D8AfQC2ufvvmmaZEE2k4fRiI/T19XkvZV3aOXSJhh4M9lXejKFLCs0YTjSbyclJTE1NUcOU1BZZIEcXWdDy9GI7YF/PVbMYEdFxmfzTTz+t+7hsKDA1NUV1U7I2bKj48ccfU132PjbjvWnVMCfJtpZYIESXIUcXWSBHF1kgRxdZcFUEoywIqjqdHcmjfDWrPoyCyXqDs+gaUoLRjz76qO7jsmuLrjclwGS2NSNATbKh8tmE6AHk6CIL5OgiC+ToIgvk6CILroqsC8siRJkFlgGYO3cu1WUZi4jBwcGSbOXKlVR3xYoVJdmyZctKsgULFtR9rigDcfbs2ZLs4MGDVPfVV18tyQ4cOEB1z58/X5JVrcCM5J1eeCFEzyBHF1kgRxdZIEcXWVApGDWztwBcADAF4JK7r22GUREpU+JRYMR0P/jgA6rLGgUtX76c6j7xxBMl2ZNPPkl1V69eXZItWrSoLhnAa8yjcgPW1OfixYtU96GHHirJXnjhBarLAtqxsTGqy2yLmjCxzycKRlPWHDQj6/K0u59pwnGEaBkauogsqOroDuAlM9tnZpubYZAQraDq0OUL7n7KzG4H8Esz+4O775mpUPwDbC4eVzydEI1R6Y7u7qeK3xMAdqLWYXe2zlZ3X+vua+XoolM0fEc3s5sAzHH3C8XjvwLwvWYZljKtH9hH5VF2gsGi+qVLlxJNPoUfZRZYdmJiYqIki5o9sUUeURlDf39/ScZKCADg8ccfL8neffddqrtjx46S7MwZnpNg9ka0quFSlSMMANhZONQ1AP7D3f+3skVCtIAqvRePAvhcE20RomUovSiyQI4usqDj9egp++lEAWZKCQALRlOmksfHx6l8//79Jdnx48epLrOXBX3nzp2jr2fBXVS7zqb1n376aap77733lmRRycPChQtLsiggZvamtK9L6UocoTu6yAI5usgCObrIAjm6yAI5usiCjmddIlKa+7NMSkqxfhS9s4b5UdaFZU2iqWs23c86DkQbCbDsBsuCAMDdd99dkt10001Ul50ves8//PDDkiwqebj++utLsujzSekVqayLELOQo4sskKOLLJCjiyzo2mC06i5tKV0AIl0WXL3//vtUlwVtkb0s6GN2RVPqLJiMOg4888wzJdnAwADVPXHiREk2MjJCdd95552SLKUN4CeffEJ1GSllIhG6o4sskKOLLJCjiyyQo4sskKOLLLhi1sXMtgH4EoAJd7+/kPUD+BmA5QDeArDR3fkqgSuQEqmnRN8pGZoou8FsSGl2H9nLzsdW669atYq+/uGHHy7JWHYFAIaGhkqy0dFRqjs8PFySvfzyy1SXrfiPNk5gpRBRuUBKOUcK9dzRfwJg/SzZFgDD7r4KwHDxXIiu5YqOXnTeml2xtAHA9uLxdgBfbrJdQjSVRieMBtx9+vtvDLUeLxS1pBPdQOVg1GsD0bBeUi3pRDfQ6B193MwG3X3UzAYBlAus6yQlGI3qu6vuSsfqpQEeeEY14iyYXLJkCdVlq+1Z0HjffffR17OWcuz8AK+T37NnD9EEdu3aVZIdPnyY6qbctNh7lrJeIKX7Q0Sjd/TdADYVjzcBKL9DQnQRV3R0M3sRwKsA7jWzk2b2NQA/ALDOzA4B+MviuRBdyxWHLu7+bPCnLzbZFiFahmZGRRbI0UUWdO3CCzYVHK1eTyniZ1H9zTffTHUvXLhQkkWr7e+///6SjG3JCACPPPJIScZ6HM6fP5++nl1DtPXh0aNHS7Io68L6R0aZLiaPsiMsqxVtecmOkZJdidAdXWSBHF1kgRxdZIEcXWRBx4PRqC6ZBSXR9DuTR3XjN954Y902sCD1jjvuoLp33XVXSbZy5Uqqe+utt5ZkFy9eLMkmJyfp61lQHgVs7FzRbnfXXnttSZYS6EfBKGtfF8E+t+i46gIgxCzk6CIL5OgiC+ToIgs6HoxGpNSYpwQlbIe0KOhji5hZcAfwQI7NSgLAoUOHSjIWjLLAGeBBMuuDDgArVqwoydatW0d133zzzZLstddeo7osSGXvAcA/t2jGNWXxeQq6o4sskKOLLJCjiyyQo4sskKOLLGi0Jd13AfwdgNOF2vPu/otWGTkNy5gAaSvHWbYgmuZmGYBz53jnvdOnT5dkx44do7pHjhwpyVjmJypNYGUI69fPbqZWg7W1W7t2LdVlre727dtHdVn7uWgHO5Z1aca0fgqNtqQDgH909zXFT8udXIgqNNqSToieosoY/Tkz+62ZbTOz2yIlM9tsZnvNbG8zlkQJ0QiNOvqPAKwEsAbAKIAfRopqSSe6gYZKANz9T/uEm9mPAfxPowY04y7Ppp7ZlDqQVu/M6qhPnTpFddludSdPnqS6bOv1qEc7gwW+99xzD9VlQW5ULrB69eqSLGrXV3VL94hOBqMlin6L03wFwBvNMUeI1lBPevFFAE8BWGhmJwF8B8BTZrYGtS66bwH4egttFKIyjbak+9cW2CJEy9DMqMgCObrIgo4vvIimuVl2JCrKT9nJjOmmTF2fP3+e6r733nslWZT5YbYxWcr1nj17luqy8oYok8IWdESLKRjRZxm9v1Vpx0YAQvQUcnSRBXJ0kQVydJEFHQ9Go4CLBTDRtudsijnqLT4xUd5ALzouI2U3tSjoY0HUDTfcUJJFU+dMHm2nfsstt5RkUZDM+sFH256nBM8puiygjT4ftaQTYhZydJEFcnSRBXJ0kQVydJEFHc+6RLCoPOpFuGTJkpKM9RwEgDNnzpRko6OjVJet+I+mnVnWJcossKwJy47ceeed9PWPPvpoSRZ1AWAbFEQ9Id94o7ysoBmdF9h7FpULpJRzzLbhs7IwuqOLLJCjiyyQo4sskKOLLKhnzehSAP8GYAC1NaJb3f2fzawfwM8ALEdt3ehGd+f92hqABXLRVuaLFy8uyaKV7g888EBJFm3X/fbbb5dkx48fp7psZf/tt99OdRctWlSSLViwoCRjLeIA4MEHHyzJli1bRnXHx8dLsldeeYXqjoyMUDmDBaMpdf1RUM8+92jTgNnH+Kz69Hru6JcAfNvdhwB8HsA3zGwIwBYAw+6+CsBw8VyIrqSelnSj7v568fgCgAMAFgPYAGB7obYdwJdbZaQQVUnKo5vZcgAPAXgNwIC7Tyegx1Ab2rDXbAawuXjcqJ1CVKLuYNTM5gH4TwDfcvc/WzjptcERHSCpJZ3oBupydDO7FjUn/3d3/69CPD7dsav4XS70FqJLqCfrYqg1LDrg7v8w40+7AWwC8IPi965mGsbu/lF2hEXl0TaJrEdhlM1h/RSjLgDMXtawHwD6+/tLsttuKzckHhigo0E6fX7ixAmq+9JLL5VkO3bsoLpsW8gok3HdddeVZCkLWCJa9a1fzxj9LwD8LYD9ZvabQvY8ag7+czP7GoDjADa2xEIhmkA9Lel+DSD6N/tic80RojVoZlRkgRxdZEHH69GjAIYFXNGKdDbNPTY2RnVZjffg4CDR5GUE0cp8Vis/b948qsvKBVLa37GgcefOnVSXBaPs/QL4tUXXy+yNdvdL6ehQb7u+VHRHF1kgRxdZIEcXWSBHF1kgRxdZ0Pasy+xoPVoNziLtKFI/cuRISca2TgR4f8EoQzM0NFSSLVy4kOqyrEvUXYAt3mAdB9h1AXyBxLFjx6gu2xKRTd8DPMMSdTJgGZYoO8KOkbJIoxnoji6yQI4uskCOLrJAji6ywFo1+Gf09fV5NC1eD1FgxIgCVxYERTuvpeiy4DdquM/avLFALlr9zgL4ZuzCxwLqyIbo2hjsfJHfVZnun5ycxNTUFD2A7ugiC+ToIgvk6CIL5OgiC67o6Ga21Mz+38x+b2a/M7NvFvLvmtkpM/tN8fM3rTdXiMa4YtalaGUx6O6vm9nNAPah1pVrI4BJd//7ek9WNesSwa4hyrqkNJpPgR03aqLPzsem36MMD9ONriElO8JImaqPdFNoVdalnsXRowBGi8cXzGy6JZ0QPUPSv+CslnQA8JyZ/dbMtplZuTFJ7TWbzWyvme1tZ85eiJlUaUn3IwArAaxB7Y7/Q/Y6taQT3UDDLencfdzdp9z9MoAfA3isdWYKUY2GW9KZ2eCMbrpfAVDe0qxNsG+KqM6dBUzRNHdKyQGr+04JRlN2eZs/f35JxursU2HvQ0ppQcqudO3+dq/Sku5ZM1uDWhfdtwB8vSUWCtEEqrSk+0XzzRGiNWhmVGSBHF1kgRxdZEFPLbxoRlTfqutlGZqUc7EsUZT1SVmkwTI/0VR9SpapG+dEtPBCZI8cXWSBHF1kgRxdZEFbg1EzO43axl4AsBDAmbadvH3oujrHXe6+iP2hrY7+Zyeule2u7cjJW4iuqzvR0EVkgRxdZEEnHX1rB8/dSnRdXUjHxuhCtBMNXUQWyNFFFrTd0c1svZkdNLPDZral3edvJkX3gwkze2OGrN/Mfmlmh4rftDtCN/MZTat69tra6uhm1gfgXwD8NYAh1JbjlTcK6h1+AmD9LNkWAMPuvgrAcPG817gE4NvuPgTg8wC+UXxOPXtt7b6jPwbgsLsfdfdPAPwUwIY229A03H0PgHdniTcA2F483o5aV7Oewt1H3f314vEFANNNq3r22trt6IsBnJjx/CSuvq5fAzO6I4wBGOikMVWZ1bSqZ69NwWgL8Vrutmfzt6Rp1Z/otWtrt6OfArB0xvMlhexqYrxozDrdoHWiw/Y0BGtahR6+tnY7+giAVWa2wszmAvgqgN1ttqHV7AawqXi8CcCuDtrSEFHTKvTwtbV9ZrToo/5PAPoAbHP377fVgCZiZi8CeAq1EtZxAN8B8N8Afg5gGWolyRvdfXbA2tWY2RcA/ArAfgDTC0mfR22c3pPXphIAkQUKRkUWyNFFFsjRRRbI0UUWyNFFFsjRRRbI0UUW/BHNCgaS67wy2wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_image([encoded_images[215]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ae.save(\"models/mnist-numbers\")\n",
    "len(images[0].shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
